{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.sparse import lil_matrix\n",
    "\n",
    "import pickle\n",
    "import nltk\n",
    "from sklearn.metrics import classification_report, f1_score, accuracy_score, confusion_matrix\n",
    "from unicodedata import normalize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#concatenate\n",
    "def concat(*args):\n",
    "    strs = [str(arg) for arg in args]\n",
    "    return ','.join(strs) if strs else np.nan\n",
    "\n",
    "#remove portuguese characters\n",
    "def remove_special(txt, codif='latin-1'):\n",
    "    return normalize('NFKD', txt.decode(codif)).encode('ASCII','ignore')\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    from doctest import testmod\n",
    "    testmod()\n",
    "    \n",
    "#create vocablist from category    \n",
    "def get_token(text):\n",
    "    stopwords = nltk.corpus.stopwords.words('portuguese')\n",
    "    stemmer = nltk.stem.RSLPStemmer()\n",
    "    aux = nltk.word_tokenize(text)\n",
    "    freq = []\n",
    "    for w in aux:\n",
    "        if w not in stopwords and len(w)>2:\n",
    "            freq.append(w)\n",
    "    #for i in range(len(freq)):\n",
    "        #freq[i] = stemmer.stem(freq[i])\n",
    "    freq = nltk.FreqDist(freq)\n",
    "    vocablist = []\n",
    "    for w,f in freq.most_common():\n",
    "        if f>0:\n",
    "            vocablist.append(w)\n",
    "    #vocablist = freq.items()\n",
    "    return vocablist\n",
    "\n",
    "#return the respective vector when comparing to vocablist\n",
    "def get_vector(text, tokens):\n",
    "    vec = lil_matrix((len(text),len(tokens)))\n",
    "    for j in range(len(text)):\n",
    "        ca = get_token(text[j])\n",
    "        for i in range(len(tokens)):\n",
    "            if tokens[i] in ca:\n",
    "                vec[j,i] = 1\n",
    "    return vec\n",
    "\n",
    "def get_models(category,y,vocab_known,app):\n",
    "    models = []\n",
    "    for column in category:\n",
    "        text = ' '.join([''.join(sentence) for sentence in vocab_known[column][app]])\n",
    "        token = get_token(text)\n",
    "        clf = LogisticRegression()\n",
    "        X = get_vector(category[column],token)\n",
    "        clf.fit(X,y)\n",
    "        models.append(clf)\n",
    "    return models\n",
    "\n",
    "def get_vocabs(data, ind):\n",
    "    vocabs = []\n",
    "    for column in data:\n",
    "        text = ' '.join([''.join(sentence) for sentence in data[column][ind]])\n",
    "        token = get_token(text)\n",
    "        vocabs.append(token)\n",
    "    return vocabs\n",
    "\n",
    "def extra_remove(text):\n",
    "    text = text.replace('\\xc3','')\n",
    "    text = text.replace('\\x89','')\n",
    "    text = text.replace('\\xa7','')\n",
    "    text = text.replace('\\xa1','')\n",
    "    text = text.replace('\\xa3','')\n",
    "    text = text.replace('\\xad','')\n",
    "    text = text.replace('\\xaa','')\n",
    "    text = text.replace('\\xa9','')\n",
    "    text = text.replace('\\xb5','')\n",
    "    text = text.replace('\\xb3','')\n",
    "    text = text.replace('\\xa0','')\n",
    "    text = text.replace('\\xb4','')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Load general data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "path  = '/Users/andremendes/OneDrive/Estudar/Estudar Projects/JFS/Data-Analysis-JFS-Matlab-Python/Data/'\n",
    "general = pd.read_csv(path+'text_data_csv.csv')\n",
    "teste = pd.read_csv(path+'index_unknown.csv')\n",
    "index = list(teste.values.flatten()) #get the index of unknown data\n",
    "labels = general.Result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Concatenate all data per category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "category = pd.DataFrame(columns = ['extra activities','honors','life project', 'all text'])\n",
    "np_concat = np.vectorize(concat)\n",
    "\n",
    "#initilialize\n",
    "aux = np.empty(len(general),dtype=str)\n",
    "\n",
    "#concatenate all text from each category in column vectors\n",
    "for i in range(1,6):\n",
    "    aux = np_concat(aux, general.iloc[:,i])\n",
    "category['extra activities'] = aux;\n",
    "aux = np.empty(len(general),dtype=str)\n",
    "for i in range(7,9):\n",
    "    aux = np_concat(aux, general.iloc[:,i])\n",
    "category['honors'] = aux;\n",
    "aux = np.empty(len(general),dtype=str)\n",
    "for i in range(1,10):\n",
    "    aux = np_concat(aux, general.iloc[:,i])\n",
    "category['all text'] = aux;   \n",
    "category['life project'] = np.array(general.iloc[:,9], dtype=str)\n",
    "\n",
    "#remove special caracters\n",
    "for column in category:\n",
    "    category[column] = category[column].apply(lambda x: remove_special(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "source": [
    " ## Create Vocabularies and Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#data from approved candidates\n",
    "#vocab_app_known = category.drop(index)\n",
    "#vocab_labels = labels.drop(index)\n",
    "#app = vocab_labels[vocab_labels==1].index\n",
    "#vocabs = get_vocabs(vocab_app_known,app)\n",
    "#with open(path+'vocab_app.pkl', 'wb') as fid: pickle.dump(vocabs, fid)\n",
    "    \n",
    "#models = get_models(category,y,vocab_app_known,app)\n",
    "#with open(path+'models.pkl', 'wb') as fid: pickle.dump(models, fid)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load models and vocabularies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = labels\n",
    "scores = pd.DataFrame(columns = ['extra activities','honors','life project', 'all text'])\n",
    "with open(path+'vocab_app.pkl', 'rb') as fid: vocabs = pickle.load(fid)\n",
    "with open(path+'models.pkl', 'rb') as fid: models = pickle.load(fid)\n",
    "i=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for column in category:    \n",
    "    #get scores\n",
    "    vocab = vocabs[i]\n",
    "    clf = models[i]\n",
    "    X = get_vector(category[column],vocab)\n",
    "    predicted = clf.predict_proba(X)\n",
    "    scores[column] = predicted[:,1]\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scores.to_csv(path+\"text_scores.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
